# -*- coding: utf-8 -*-
"""Assignment_5_pytorch_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11J-mgKfSbnRyUSssMCulyx6Vo2Yk_7wP

Import and setup some auxiliary functions
"""

# Don't edit this cell
import torch
from torchvision import transforms, datasets
import numpy as np
import timeit
from collections import OrderedDict
from pprint import pformat
from torch.utils.data.sampler import *
from tqdm import tqdm
import time
from google.colab import drive

import torch.nn as nn
import torch.nn.functional as F

torch.multiprocessing.set_sharing_strategy('file_system')

def compute_score(acc, min_thres, max_thres):
    if acc <= min_thres:
        base_score = 0.0
    elif acc >= max_thres:
        base_score = 100.0
    else:
        base_score = float(acc - min_thres) / (max_thres - min_thres) \
                     * 100
    return base_score


def run(algorithm, dataset_name, filename):
    predicted_test_labels, gt_labels, run_time = algorithm(dataset_name)
    if predicted_test_labels is None or gt_labels is None:
      return (0, 0, 0)

    correct = 0
    total = 0
    for label, prediction in zip(gt_labels, predicted_test_labels):
      total += label.size(0)
      correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()   # assuming your model runs on GPU
      
    accuracy = float(correct) / total
    
    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))
    return (correct, accuracy, run_time)

"""Your implementation starts here"""

# Part[1] TODO: Cifar-10 dataloading
def load_data(dataset_name, device, config):
    """
    loads cifar-10 dataset using torchvision, take the last 5k of the training data to be validation data
    """
    if dataset_name == "CIFAR10":
        CIFAR10_training = datasets.CIFAR10("/CIFAR10_dataset/",train=True, download=True, transform=config['transforms'])
          
        CIFAR10_test = datasets.CIFAR10("/CIFAR10_dataset/",train=False, download=True, transform=config['transforms'])
          
        # CIFAR10_training_dataset, CIFAR10_validation_dataset = random_split(CIFAR10_training, [45000, 5000])
        dataset_size = len(CIFAR10_training)
        indices = list(range(dataset_size))
        split = int(np.floor(0.1 * dataset_size))
        train_indices, valid_indices = indices[split:], indices[:split]
        CIFAR10_training_sampler = SubsetRandomSampler(train_indices)
        CIFAR10_validation_sampler = SubsetRandomSampler(valid_indices)

        train_dataloader  = torch.utils.data.DataLoader(CIFAR10_training, batch_size=config['batch_size'], sampler=CIFAR10_training_sampler, num_workers=2)
        valid_dataloader  = torch.utils.data.DataLoader(CIFAR10_training, batch_size=config['batch_size'], sampler=CIFAR10_validation_sampler, num_workers=2)
        test_dataloader = torch.utils.data.DataLoader(CIFAR10_test)
        
    return train_dataloader, valid_dataloader, test_dataloader

# Part [2] TODO: Main model definition + any utilities such as weight initialization
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()    
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=5, stride=1, padding=2) #padding=(kernelsize-stride)/2
        self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=5, stride=1, padding=2)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc1 = nn.Linear(in_features=16 * 8 * 8, out_features=256)
        self.fc2 = nn.Linear(in_features=256, out_features=84)
        self.fc3 = nn.Linear(in_features=84, out_features=10)
        
    def forward(self, x):
        x = F.relu(self.conv1(x)) # [12, 32, 32]
        x = self.pool(x) # [12, 16, 16]
        x = F.relu(self.conv2(x)) #[16, , 16]
        x = self.pool(x) # [16, 8, 8]
        x = x.view(-1, 16 * 8 * 8)  # reshape tensor
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        x = self.fc3(x)
        return x

# Part [3] TODO : Main trainig + validation, returns a trained model
def train(train_dataloader, valid_dataloader, device, config):
    examples = enumerate(train_dataloader)
    _, (example_data, _) = next(examples)
    print(example_data.shape)

    model = Net().to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['regular_constant'])

    for epoch in range(1, config['num_epochs']+1):
        model.train()
        for i, (images, labels) in enumerate(train_dataloader): 
            images = images.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        
        # validation 
        if epoch%5== 0:
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for images, labels in valid_dataloader:
                    images = images.to(device)
                    labels = labels.to(device)
                    outputs = model(images)
                    _, pred = torch.max(outputs.data, 1)
                    correct += (pred == labels).sum().item()
                    total += labels.size(0)

            accuracy = 100. * correct / total
            print("epoch: {}. Accuracy: {:.2f}.".format(epoch, accuracy))
                  
    return model

# Part [4] TODO: Testing + paramater setting
# TODO: Testing + paramater setting

def save_model_colab_for_submission(model):  # if you are running on colab
  drive.mount('/content/gdrive/', force_remount=True)
  
  torch.save(model.to(torch.device("cpu")), '/content/gdrive/My Drive/model.pt') # you will find the model in your home drive
  

def save_model_local_for_submission(model):  # if you are running on your local machine
  torch.save(model.to(torch.device("cpu")), 'model.pt')
  
def test(model, test_dataloader, device):
  test_predictions = []
  true_labels = []
  model.eval()
  with torch.no_grad():
      for images, labels in test_dataloader:
          images = images.to(device)
          labels = labels.to(device)
          outputs = model(images)
          _, pred = torch.max(outputs.data, 1)  
          # pred = pred.cpu().numpy()
          # labels = labels.cpu().numpy()
          test_predictions.append(pred)
          true_labels.append(labels)

  return test_predictions, true_labels

def run_NN(dataset_name):
    # set parameters cifar10
  config = {
        'lr': 0.001, #adam,
        'num_epochs': 20,
        'batch_size': 128,
        'num_classes': 10,
        'regular_constant': 5e-3,
        'transforms': transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) }
    
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  
  train_dataloader, valid_dataloader, test_dataloader = load_data(dataset_name, device, config)
  
  model = train(train_dataloader, valid_dataloader, device, config)
         
  device = torch.device("cpu")
  start_time = timeit.default_timer()
  preds, labels = test(model.to(device), test_dataloader, device)
  end_time = timeit.default_timer()
  

  test_time = (end_time - start_time)
  print("Total run time of testing the model: ", test_time , " seconds.")
  
  save_model_colab_for_submission(model)
  
  return preds, labels, test_time

"""Main loop. Run time and total score will be shown below."""

# Don't edit this cell
def run_on_dataset(dataset_name, filename):
    min_thres = 0.28
    max_thres = 0.38

    correct_predict, accuracy, run_time = run(run_NN, dataset_name, filename)

    score = compute_score(accuracy, min_thres, max_thres)
    result = OrderedDict(correct_predict=correct_predict,
                         accuracy=accuracy,
                         run_time=run_time)
    return result, score


def main():
    filenames = { "CIFAR10": "predictions_cifar10_zijunwu_1488834.txt"}
    result_all = OrderedDict()
    score_weights = [0.5]
    scores = []
    for dataset_name in ["CIFAR10"]:
        result_all, this_score = run_on_dataset(dataset_name, filenames[dataset_name])
    with open('result.txt', 'w') as f:
        f.writelines(pformat(result_all, indent=4))
    print("\nResult:\n", pformat(result_all, indent=4))


main()