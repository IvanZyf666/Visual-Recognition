# -*- coding: utf-8 -*-
"""Assignment_6_notebook_.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UqM-uY5syVVVrflH5Uaj6hi1qnMuXuG2

**Import** *and* setup some auxiliary functions
"""

# Don't edit this cell
import os
import timeit
import time
import numpy as np
from collections import OrderedDict
from pprint import pformat
from tqdm import tqdm
from google.colab import drive

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
from torch.utils.data.sampler import *
from torchvision import transforms, datasets

torch.multiprocessing.set_sharing_strategy('file_system')
cudnn.benchmark = True

# TODO: Main model definition + any utilities such as weight initialization or custom layers, ADD DROPOUT, BATCHNORM, SKIP CONNECTION,
class BasicBlock(nn.Module):
    def __init__(self, in_channels, out_channels, stride):
        super(BasicBlock, self).__init__()
        self.layer = nn.Sequential()
        self.layer.add_module("Conv", nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1))
        self.layer.add_module("Bn", nn.BatchNorm2d(out_channels))

        self.skip = nn.Sequential()
        if stride != 1 or in_channels != out_channels:
            self.skip = nn.Sequential()
            self.skip.add_module("Conv", nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=stride, padding=1))
            self.skip.add_module("Bn", nn.BatchNorm2d(out_channels))

    def forward(self, x):
        out = self.layer(x)
        out += self.skip(x)
        return F.relu(out)

class ResNet(nn.Module):
    def __init__(self, block, num_class=10):
        super(ResNet, self).__init__()

        self.layer1 = nn.Sequential()
        self.layer1.add_module("Conv", nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, padding=1))
        self.layer1.add_module("Bn", nn.BatchNorm2d(32))
        self.layer1.add_module("Relu", nn.ReLU())

        self.pool = nn.MaxPool2d(kernel_size=4, stride=4)

        self.layer2 = nn.Sequential(
            block(32, 32, 1),
            block(32, 32, 1),
        )
        self.layer3 = nn.Sequential(
            block(32, 64, 2),
            block(64, 64, 1),
        )

        self.layer4 = nn.Sequential(
            block(64, 128, 2),
            block(128, 128, 1),
        )

        self.linear1 = nn.Sequential(
            nn.Dropout(p=0.1),
            nn.Linear(512, num_class),
        )

    def forward(self, x):
      x = self.layer1(x)
      x = self.layer2(x)
      x = self.layer3(x)
      x = self.layer4(x)
      x = self.pool(x)
      x = self.linear1(x.view(x.size(0), -1))
      
      return x

# TODO: Cifar-10 dataloading
def load_data(config):
    """
    Load cifar-10 dataset using torchvision, take the last 10k of the training data to be validation data
    """
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    CIFAR10_training = datasets.CIFAR10("/CIFAR10_dataset/",train=True, download=True, transform=transform_train)      
    CIFAR10_test = datasets.CIFAR10("/CIFAR10_dataset/",train=False, download=True, transform=transform_test)
    
    dataset_size = len(CIFAR10_training)
    indices = list(range(dataset_size))
    split = int(np.floor(0.1 * dataset_size))
    train_indices, valid_indices = indices[split:], indices[:split]
    CIFAR10_training_sampler = SubsetRandomSampler(train_indices)
    CIFAR10_validation_sampler = SubsetRandomSampler(valid_indices)

    train_dataloader  = torch.utils.data.DataLoader(CIFAR10_training, batch_size=config['batch_size'], sampler=CIFAR10_training_sampler, num_workers=2)
    valid_dataloader  = torch.utils.data.DataLoader(CIFAR10_training, batch_size=config['batch_size'], sampler=CIFAR10_validation_sampler, num_workers=2)
    test_dataloader = torch.utils.data.DataLoader(CIFAR10_test)
  
    return train_dataloader, valid_dataloader, test_dataloader

# TODO : Main trainig + validation, returns the final model, save your best checkpoint based on the best validation accuracy
def train(trainloader, testloader, device, config):
    model = ResNet(BasicBlock).to(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=config['momentum'], weight_decay=config['regular_constant'])

    for epoch in range(1, config['num_epochs']+1):
        model.train()
        for _, (images, labels) in enumerate(trainloader): 
            images = images.to(device)
            labels = labels.to(device)
            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
        
        # validation 
        if epoch%5== 0:
            model.eval()
            correct = 0
            total = 0
            with torch.no_grad():
                for images, labels in testloader:
                    images = images.to(device)
                    labels = labels.to(device)
                    outputs = model(images)
                    _, pred = torch.max(outputs.data, 1)
                    correct += (pred == labels).sum().item()
                    total += labels.size(0)

            accuracy = 100. * correct / total
            print("epoch: {}. Accuracy: {:.2f}.".format(epoch, accuracy))

    return model

def save_model_colab_for_submission(model):  # if you are running on colab
  drive.mount('/content/gdrive/', force_remount=True)
  torch.save(model.to(torch.device("cpu")), '/content/gdrive/My Drive/model.pt') # you will find the model in your home drive
  
def save_model_local_for_submission(model):  # if you are running on your local machine
  torch.save(model.to(torch.device("cpu")), 'model.pt')

#TODO: Implement testing
def test(net, testloader, device):
    correct = 0
    total = 0
    net.eval()
    with torch.no_grad():
        for images, labels in testloader:
            images = images.to(device)
            labels = labels.to(device)
            outputs = net(images)
            _, pred = torch.max(outputs.data, 1)
            correct += (pred == labels).sum().item()
            total += labels.size(0)

    ###
    return 100.*correct/total, correct, total

def run():
  # set parameters cifar10
  config = {
        'lr': 0.001,
        'num_epochs': 20,
        'batch_size': 128,
        'num_classes': 10,
        'momentum':0.97,
        'regular_constant': 5e-3,
       }
    
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  
  train_dataloader, valid_dataloader, test_dataloader = load_data(config)
  
  model = train(train_dataloader, valid_dataloader, device, config)
  
  # Testing and saving for submission
  device = torch.device("cpu")
  

  try:
    assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'
  except AssertionError:
    os.mkdir('./checkpoint')

  torch.save(model.state_dict(),'./checkpoint/ckpt.pth')
  
  checkpoint = torch.load('./checkpoint/ckpt.pth')
  model.load_state_dict(checkpoint)
  model.eval()
  
  start_time = timeit.default_timer()
  test_acc, test_correct, test_total = test(model.to(device), test_dataloader, device)
  end_time = timeit.default_timer()
  test_time = (end_time - start_time)
  
  save_model_colab_for_submission(model)

  return test_acc, test_correct, test_time

"""Main loop. Run time and total score will be shown below."""

# Don't edit this cell
def compute_score(acc, min_thres=65, max_thres=8):
  # Your Score thresholds
  if acc <= min_thres:
      base_score = 0.0
  elif acc >= max_thres:
      base_score = 100.0
  else:
      base_score = float(acc - min_thres) / (max_thres - min_thres) * 100
  return base_score

def main():
    
    accuracy, correct, run_time = run()
    
    score = compute_score(accuracy)
    
    result = OrderedDict(correct=correct,
                         accuracy=accuracy,
                         run_time=run_time,
                         score=score)
    
    with open('result.txt', 'w') as f:
        f.writelines(pformat(result, indent=4))
    print("\nResult:\n", pformat(result, indent=4))


main()