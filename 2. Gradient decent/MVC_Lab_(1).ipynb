{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MVC_Lab (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hfn-Yiy1lp5l",
        "colab_type": "text"
      },
      "source": [
        "# Lab Assignment 2\n",
        "## With this assignment you will get to know more about gradient descent optimization and writing your own functions with forward and backward (i.e., gradient) passes\n",
        "## You need to complete all the tasks in this notebook in the lab and show you work to the TA. Edit only those portions in the cells where it asks you to do so!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp3BetP-d6cB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJpovSL8d_-l",
        "colab_type": "text"
      },
      "source": [
        "## Huber loss function\n",
        "https://en.wikipedia.org/wiki/Huber_loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTp4nNf9d-zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A loss function measures distance between a predicted and a target tensor\n",
        "# An implementation of Huber loss function is given below\n",
        "# We will make use of this loss function in gradient descent optimization\n",
        "def Huber_Loss(input,delta):\n",
        "  m = (torch.abs(input)<=delta).detach().float()\n",
        "  output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZoxXPadgk-O",
        "colab_type": "text"
      },
      "source": [
        "# Test Huber loss with a couple of different examples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYO_KmUQfmnm",
        "colab_type": "code",
        "outputId": "a38d1d04-a8e6-4f8c-c594-a7818a489654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "a = torch.tensor([[0.3, 2.0, -3.1],[0.5, 9.2, 0.1]])\n",
        "print(a.numpy())\n",
        "ha = Huber_Loss(a,1.0)\n",
        "print(ha.numpy())\n",
        "\n",
        "b = torch.tensor([0.3, 2.0])\n",
        "print(b.numpy())\n",
        "hb = Huber_Loss(b,1.0)\n",
        "print(hb.numpy())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.3  2.  -3.1]\n",
            " [ 0.5  9.2  0.1]]\n",
            "12.974999\n",
            "[0.3 2. ]\n",
            "1.545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26wLACTj7FkG",
        "colab_type": "text"
      },
      "source": [
        "# Gradient descent code\n",
        "## Study the following generic gradient descent optimization code.\n",
        "## Huber loss f measures the distance between a probability vector z and target 1-hot vector target.\n",
        "## When f.backward is called, PyTorch first computes $\\nabla_z f$ (gradient of f with respect to z), then by chain rule it computes $\\nabla_{var} f = J^{z}_{var} \\nabla_z f$, where $J^{z}_{var}$ is the Jacobian of z with respect to var.\n",
        "## Next, optimizer.step() call adjusts the variable var in the opposite direction of $\\nabla_{var} f.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLxQgQaD7Krq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gradient_descent(var,optimizer,softmax,loss,target,nIter,nPrint):\n",
        "  for i in range(nIter):\n",
        "    z = softmax(var)\n",
        "    f = loss(z-target,1.0)\n",
        "    optimizer.zero_grad()\n",
        "    f.backward()\n",
        "    optimizer.step()\n",
        "    if i%nPrint==0:\n",
        "      with np.printoptions(precision=3, suppress=True):\n",
        "        print(\"Iteration:\",i,\"Variable:\", z.detach().numpy(),\"Loss: %0.6f\" % f.item())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5viWaJpSiDuN",
        "colab_type": "text"
      },
      "source": [
        "# Gradient descent with Huber Loss\n",
        "## The following cell shows how gradient_descent function can be used.\n",
        "## The cell first creates a target 1-hot vector y, where only the 3rd place is on.\n",
        "## It also creates a variable x with random initialization and an optimizer.\n",
        "## Learning rate and momentum has been set to 0.1 and 0.9, respectively.\n",
        "## Then it calls gradient_descent function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzRgWv_NiIeQ",
        "colab_type": "code",
        "outputId": "41cff155-db05-4704-accb-b45e0c57df08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(\"Target 1-hot vector:\",y.numpy())\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,F.softmax,Huber_Loss,y,1000,100)\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target 1-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration: 0 Variable: [0.051 0.317 0.022 0.018 0.062 0.008 0.118 0.204 0.045 0.155] Loss: 0.572318\n",
            "Iteration: 100 Variable: [0.007 0.009 0.937 0.003 0.008 0.001 0.009 0.009 0.007 0.009] Loss: 0.002269\n",
            "Iteration: 200 Variable: [0.005 0.006 0.956 0.002 0.006 0.001 0.006 0.006 0.005 0.006] Loss: 0.001073\n",
            "Iteration: 300 Variable: [0.004 0.005 0.964 0.002 0.005 0.001 0.005 0.005 0.004 0.005] Loss: 0.000733\n",
            "Iteration: 400 Variable: [0.004 0.004 0.969 0.002 0.004 0.001 0.005 0.005 0.003 0.005] Loss: 0.000556\n",
            "Iteration: 500 Variable: [0.003 0.004 0.972 0.001 0.004 0.001 0.004 0.004 0.003 0.004] Loss: 0.000448\n",
            "Iteration: 600 Variable: [0.003 0.004 0.974 0.001 0.003 0.001 0.004 0.004 0.003 0.004] Loss: 0.000375\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 700 Variable: [0.003 0.003 0.976 0.001 0.003 0.001 0.003 0.003 0.003 0.003] Loss: 0.000322\n",
            "Iteration: 800 Variable: [0.003 0.003 0.978 0.001 0.003 0.001 0.003 0.003 0.002 0.003] Loss: 0.000283\n",
            "Iteration: 900 Variable: [0.002 0.003 0.979 0.001 0.003 0.001 0.003 0.003 0.002 0.003] Loss: 0.000252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtIf2LRqvOph",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>20% Weight:</font> In this markdown using math mode write gradient of Huber loss function: $output = \\sum_i 0.5 m_i (input)^{2}_{i} + \\delta (1-m_i)(|input_i|-0.5 \\delta)$ with respect to $input.$ Treat $m_i$ to be independent of $input_i,$ becuase we replaced if control statement with $m_i.$\n",
        "## Your solution <font color='red'>20% (complete formula)</font>: $\\frac{\\partial (output)}{\\partial (input)_i} =  m_i (input)_{i} + \\delta (1-m_i) * sign(input)_i$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly8SaBQ-lXbg",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>20% Weight:</font> Define your own (correct!) rule of differentiation for Huber loss function\n",
        "## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n",
        "## For this function chain rule (Jacobian-vector product) takes the following form: $\\frac{\\partial (cost)}{\\partial (input)_i} = \\frac{\\partial (output)}{\\partial (input)_i} \\frac{\\partial (cost)}{\\partial (output)}.$\n",
        "# In the backward method below, $\\frac{\\partial (cost)}{\\partial (output)}$ is denoted by output_grad and the ith component of input_grad is symbolized by $\\frac{\\partial (cost)}{\\partial (input)_i}.$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UX4zC76XlWr0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inherit from Function\n",
        "class My_Huber_Loss(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, delta):\n",
        "        m = (torch.abs(input)<=delta).float()\n",
        "        ctx.save_for_backward(input,torch.tensor(m),torch.tensor(delta))\n",
        "        output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, output_grad):\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        input, m, delta = ctx.saved_tensors\n",
        "\n",
        "        # Return Jacobian-vector product (chain rule)\n",
        "        # For Huber loss function the Jacobian happens to be a diagonal matrix\n",
        "        # Also, note that output_grad is a scalar, because forward function returns a scalar value\n",
        "        input_grad = (m * input + delta * (1-m) * torch.sign(input)) * output_grad # complete this line, do not use for loop\n",
        "        # must return two gradients becuase forward function takes in two arguments\n",
        "        return input_grad, None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkG5zXGZcgja",
        "colab_type": "text"
      },
      "source": [
        "#Gradient Descent on Your Own Huber Loss\n",
        "## You should get almost identical results as before if your rule of differentation is correct!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DKnFDK0pPjF",
        "colab_type": "code",
        "outputId": "5df0616f-a3e7-4143-86a3-5373d00bb3d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(\"Target:\",y.numpy())\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,F.softmax,My_Huber_Loss.apply,y,1000,100)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Target: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Iteration: 0 Variable: [0.115 0.132 0.032 0.219 0.049 0.045 0.176 0.128 0.022 0.082] Loss: 0.537294\n",
            "Iteration: 100 Variable: [0.008 0.008 0.94  0.008 0.005 0.005 0.008 0.008 0.003 0.007] Loss: 0.002031\n",
            "Iteration: 200 Variable: [0.006 0.006 0.957 0.006 0.004 0.004 0.006 0.006 0.002 0.005] Loss: 0.001055\n",
            "Iteration: 300 Variable: [0.005 0.005 0.964 0.005 0.003 0.003 0.005 0.005 0.002 0.004] Loss: 0.000727\n",
            "Iteration: 400 Variable: [0.004 0.004 0.969 0.004 0.003 0.003 0.004 0.004 0.002 0.004] Loss: 0.000554\n",
            "Iteration: 500 Variable: [0.004 0.004 0.972 0.004 0.003 0.002 0.004 0.004 0.001 0.003] Loss: 0.000447\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 600 Variable: [0.003 0.003 0.974 0.003 0.002 0.002 0.003 0.003 0.001 0.003] Loss: 0.000375\n",
            "Iteration: 700 Variable: [0.003 0.003 0.976 0.003 0.002 0.002 0.003 0.003 0.001 0.003] Loss: 0.000323\n",
            "Iteration: 800 Variable: [0.003 0.003 0.977 0.003 0.002 0.002 0.003 0.003 0.001 0.003] Loss: 0.000283\n",
            "Iteration: 900 Variable: [0.003 0.003 0.979 0.003 0.002 0.002 0.003 0.003 0.001 0.003] Loss: 0.000252\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVpTDS3daNmw",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>30% Weight:</font> In this markdown using math mode write Jacobian of softmax function: $(output)_i = \\frac{exp((input)_i)}{ \\sum_j exp((input)_j)}.$\n",
        "## Your solution (<font color='red'>show your derivation to TA</font>): \n",
        "\\begin{equation*}\n",
        "    \\frac{\\partial (output)_j}{\\partial (input)_i} = \\begin{cases}\n",
        "               (output)_i -(output)_j (output)_i,               & i = j,\\\\\n",
        "               -(output)_j (output)_i, & \\text{otherwise.}\n",
        "           \\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "Solution: $\\frac{\\partial (output)_j}{\\partial (input)_i} = diag(output) - output^\\top output$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2K4Q7ePPdfy",
        "colab_type": "text"
      },
      "source": [
        "# <font color='red'>30% Weight:</font> Your own softmax with forward and backward functions\n",
        "## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n",
        "## The Jacobian-vector product (chain rule) takes the following form using summation sign: $\\frac{\\partial (cost)}{\\partial (input)_i} = \\sum_j \\frac{\\partial (output)_j}{\\partial (input)_i} \\frac{\\partial (cost)}{\\partial (output)_j}$\n",
        "# Once again note that in the backward method below, ith component of input_grad and jth component of output_grad is denoted by $\\frac{\\partial (cost)}{\\partial (input)_i}$ and $\\frac{\\partial (cost)}{\\partial (output)_j}$, respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zn52-xK_PijV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Inherit from Function\n",
        "class My_softmax(Function):\n",
        "\n",
        "    # Note that both forward and backward are @staticmethods\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = F.softmax(input,dim=0)\n",
        "        ctx.save_for_backward(output) # this is the only tensor you will need to save for backward function\n",
        "        return output\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = ctx.saved_tensors[0]\n",
        "        # retrieve saved tensors and use them in derivative calculation\n",
        "        # return Jacobian-vecor product\n",
        "        grad_input = torch.sum((torch.diag(output) - torch.ger(output, output)) * grad_output, dim = 1) # Complete this line\n",
        "        return grad_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcixVFs4cwHO",
        "colab_type": "text"
      },
      "source": [
        "# Gradient Descent on your own Huber Loss and your own softmax"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UejqQeb4RZk0",
        "colab_type": "code",
        "outputId": "0ebcda34-b59d-46ae-cbe5-b475323350a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "y = torch.zeros(10)\n",
        "y[2] = 1.0\n",
        "print(y)\n",
        "x = Variable(torch.randn(y.shape),requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n",
        "\n",
        "gradient_descent(x,optimizer,My_softmax.apply,My_Huber_Loss.apply,y,1000,100)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n",
            "tensor([ 0.8756, -1.4361, -0.6156,  0.2791, -0.2809, -0.3099,  1.8846,  0.9310,\n",
            "        -1.4725,  0.6733], requires_grad=True)\n",
            "Iteration: 0 Variable: [0.139 0.014 0.031 0.076 0.044 0.042 0.381 0.147 0.013 0.113] Loss: 0.573427\n",
            "Iteration: 100 Variable: [0.009 0.002 0.943 0.008 0.006 0.006 0.008 0.009 0.002 0.009] Loss: 0.001859\n",
            "Iteration: 200 Variable: [0.006 0.002 0.958 0.006 0.004 0.004 0.006 0.006 0.002 0.006] Loss: 0.001008\n",
            "Iteration: 300 Variable: [0.005 0.001 0.965 0.005 0.004 0.004 0.005 0.005 0.001 0.005] Loss: 0.000702\n",
            "Iteration: 400 Variable: [0.005 0.001 0.969 0.004 0.003 0.003 0.004 0.005 0.001 0.005] Loss: 0.000538\n",
            "Iteration: 500 Variable: [0.004 0.001 0.972 0.004 0.003 0.003 0.004 0.004 0.001 0.004] Loss: 0.000436\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Iteration: 600 Variable: [0.004 0.001 0.975 0.003 0.003 0.003 0.004 0.004 0.001 0.004] Loss: 0.000367\n",
            "Iteration: 700 Variable: [0.004 0.001 0.976 0.003 0.002 0.002 0.003 0.004 0.001 0.003] Loss: 0.000316\n",
            "Iteration: 800 Variable: [0.003 0.001 0.978 0.003 0.002 0.002 0.003 0.003 0.001 0.003] Loss: 0.000278\n",
            "Iteration: 900 Variable: [0.003 0.001 0.979 0.003 0.002 0.002 0.003 0.003 0.001 0.003] Loss: 0.000248\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}